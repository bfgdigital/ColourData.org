{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Display options\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# Jupytper Config\n",
    "%config Completer.use_jedi = False\n",
    "%config IPCompleter.greedy=True\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# SKLearn\n",
    "from sklearn.preprocessing import StandardScaler # good bet, but max min may be better.\n",
    "from sklearn.preprocessing import RobustScaler # will ignore outliers, but there shouldn't be any.\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN # clustering algos\n",
    "from sklearn import metrics # for results\n",
    "\n",
    "# SQL\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine('postgres://zfgtcbuzkpjiod:b52325b1a9244f1d3cf8500d14ae165af1369d7eff7841f306746f6a4e0733f4@ec2-52-20-248-222.compute-1.amazonaws.com:5432/d37uou3e7j5m64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_sql('colour_data', engine)\n",
    "df = pd.read_csv('./Notebooks/CSV/colour_data_new.csv')\n",
    "\n",
    "df = df.dropna(axis=0) # Drop nulls\n",
    "df = df.groupby('user').filter(lambda x: len(x) > 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "n_samples=1 should be >= n_clusters=3.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-a9d65e4774d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcluster_loops\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m     \u001b[0mkm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m     \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Kmeans_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0mall_clusters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1033\u001b[0m                                 accept_large_sparse=False)\n\u001b[1;32m   1034\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1035\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1036\u001b[0m         \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_random_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py\u001b[0m in \u001b[0;36m_check_params\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    956\u001b[0m         \u001b[0;31m# n_clusters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 958\u001b[0;31m             raise ValueError(f\"n_samples={X.shape[0]} should be >= \"\n\u001b[0m\u001b[1;32m    959\u001b[0m                              f\"n_clusters={self.n_clusters}.\")\n\u001b[1;32m    960\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: n_samples=1 should be >= n_clusters=3."
     ]
    }
   ],
   "source": [
    "# build user list\n",
    "user_list = list(df['user'].unique())\n",
    "user_score = []\n",
    "\n",
    "for i in df['user'].unique():\n",
    "    isolated_user = df[df['user'] == i] # Create stats mask for individual stats for each iteration\n",
    "    user_score.append(isolated_user['correct'].groupby(df['pallet_used']).mean())\n",
    "\n",
    "user_scores = pd.DataFrame(user_score, index=user_list) # make a user sorted dataframe.\n",
    "\n",
    "# find correlations\n",
    "correlations = user_scores.T # make a second transposed dataframe for heatmap\n",
    "correlations.columns = user_list # assign columns\n",
    "correlations = correlations.corr().sort_index(ascending=True) # for 50% chart.\n",
    "\n",
    "# create pallet list df\n",
    "pallet_list = list(df.pallet_used.unique())\n",
    "pallet_avg = []\n",
    "\n",
    "for i in df.pallet_used.unique():\n",
    "    pallet_stats = df[df['pallet_used'] == i] # individual stats for each iteration\n",
    "    pallet_avg.append(pallet_stats['correct'].groupby(df['user']).mean())\n",
    "\n",
    "pallet_avg = pd.DataFrame(pallet_avg, index=pallet_list) # make a user sorted dataframe.\n",
    "\n",
    "# Add identfiers to columns\n",
    "user_scores = user_scores.add_prefix('usr_scores_').add_suffix('_mean')\n",
    "correlations = correlations.add_suffix('_corr') # reset columns names. No longer diagnoally aligned.\n",
    "\n",
    "# Feature Engineering\n",
    "# Group 1 - Sums of user responses per pallet category.\n",
    "cb1_responses = []\n",
    "cb2_responses = []\n",
    "ncb_responses = []\n",
    "\n",
    "# Group 2 - Means of user responses per pallet category.\n",
    "cb1_percent_correct = []\n",
    "cb2_percent_correct = []\n",
    "ncb_percent_correct = []\n",
    "\n",
    "# Group 3 - Score & Count (of binary responses)\n",
    "percent_correct = []\n",
    "responses = []\n",
    "\n",
    "# Group 4 - list of correct responses by pallet.\n",
    "user_sums = []\n",
    "\n",
    "for i in df.user.unique():\n",
    "    isolated_user = df[df['user'] == i] # make user mask for stats.\n",
    "    \n",
    "    # Group 1 - Sum of user responses per pallet category.\n",
    "    cb1_responses.append(isolated_user['cb_type1'].sum()) # cb1 type total correct\n",
    "    cb2_responses.append(isolated_user['cb_type2'].sum()) # cb2 type total correct\n",
    "    ncb_responses.append(isolated_user['ncb'].sum()) # NCB type total correct\n",
    "    \n",
    "    # Group 2 - Mean of user responses per pallet category.\n",
    "    cb1_percent_correct.append(isolated_user['cb_type1'].mean())\n",
    "    cb2_percent_correct.append(isolated_user['cb_type2'].mean())\n",
    "    ncb_percent_correct.append(isolated_user['ncb'].mean())\n",
    "    \n",
    "    # Group 3 - Score & Count (of binary responses)\n",
    "    percent_correct.append(isolated_user['correct'].mean()) # % (float) of correct responses\n",
    "    responses.append(isolated_user['correct'].value_counts().sum()) # number of correct responses\n",
    "    \n",
    "    # Group 4 - list of correct responses by pallet.\n",
    "    user_sums.append(isolated_user['correct'].groupby(df['pallet_used']).sum())\n",
    "\n",
    "# Group 1 - Sums\n",
    "user_scores['cb1_responses'] = cb1_responses\n",
    "user_scores['cb2_responses'] = cb2_responses\n",
    "user_scores['ncb_responses'] = ncb_responses\n",
    "    \n",
    "# Group 2 - Means\n",
    "user_scores['cb1_percent_correct'] = cb1_percent_correct\n",
    "user_scores['cb2_percent_correct'] = cb2_percent_correct\n",
    "user_scores['ncb_percent_correct'] = ncb_percent_correct\n",
    "\n",
    "# Group 3 - Score & Count\n",
    "user_scores['percent_correct'] = percent_correct\n",
    "user_scores['responses'] = responses\n",
    "\n",
    "# Group 4 - Totals per pallet\n",
    "user_sums = pd.DataFrame(user_sums, index=user_list)\n",
    "user_sums = user_sums.add_suffix('_sum') # identifyable as sum value. (Needs to be scaled!)\n",
    "\n",
    "X = pd.concat([user_sums,user_scores,correlations], axis=1) # add all new dataframes together as X.b\n",
    "X['user']= user_list\n",
    "\n",
    "X = X.reindex(sorted(X.columns, reverse=True), axis=1) # Sort columns so User is index\n",
    "\n",
    "# Write to CSV/SQL\n",
    "# X.to_sql('colour_users', engine, if_exists = 'replace')\n",
    "\n",
    "# Step 2\n",
    "X.index = X['user']\n",
    "Xusers = X[['user']].copy() # re-inserted later.\n",
    "X.drop('user', axis=1, inplace=True)\n",
    "Xss = StandardScaler().fit_transform(X) # Kmeans 0.325 DBSCAN 0.370 AG 0.392\n",
    "Xrs = RobustScaler().fit_transform(X) # Kmeans 0.387 DBSCAN 0.416 AG 0.443\n",
    "\n",
    "Xss = StandardScaler().fit_transform(X) # Kmeans 0.325 DBSCAN 0.370 AG 0.392\n",
    "Xrs = RobustScaler().fit_transform(X) # Kmeans 0.387 DBSCAN 0.416 AG 0.443\n",
    "\n",
    "baseline = X['percent_correct'].mean() # Define Baseline\n",
    "\n",
    "all_clusters = pd.DataFrame(None,index = X.index) # Dataframe for recording cluster labels\n",
    "\n",
    "cluster_loops = [3,4,5] # approx 3-5 types are expected. This will be revised.\n",
    "Kmeans_results = pd.DataFrame()\n",
    "\n",
    "for value in cluster_loops:\n",
    "    km = KMeans(n_clusters=value).fit(Xrs)\n",
    "    name = 'Kmeans_' + str(value)\n",
    "    all_clusters[name] = km.labels_\n",
    "\n",
    "neigh = NearestNeighbors(n_neighbors=3)\n",
    "nbrs = neigh.fit(Xrs)\n",
    "distances, indices = nbrs.kneighbors(Xrs)\n",
    "distances = np.sort(distances, axis=0) # plotting shows approximate 85% shift.\n",
    "\n",
    "eps_val =  np.quantile(distances, .85,).mean() # 85th quantile for distances. (Assume 85% regular vision based on previous data)\n",
    "min_samples_val = 1 # 90% of the total pool required to make 1 cluster.\n",
    "\n",
    "dbscan = DBSCAN(eps=eps_val, min_samples=min_samples_val, metric = 'euclidean').fit(Xrs) # eps with highest Silhouette score\n",
    "all_clusters['DBSCAN'] = dbscan.labels_ # Assign Labels to \n",
    "    \n",
    "# Add 1 to all values so 0 values are included.\n",
    "for col in all_clusters.columns:\n",
    "    if col in all_clusters.columns:\n",
    "        all_clusters[col] = all_clusters[col] + 1\n",
    "\n",
    "# cluster interated feature lables.\n",
    "scaled_results = StandardScaler().fit_transform(all_clusters) #all_clusters.drop('DBSCAN',axis=1)\n",
    "cluster_feature = DBSCAN(eps=2, min_samples=1, metric = 'euclidean').fit(scaled_results)\n",
    "all_clusters['cluster_feature'] = cluster_feature.labels_ # Assign Labels to all_clusters\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "\n",
    "# Unsupervised Feature\n",
    "X = pd.concat([X,all_clusters['cluster_feature']],axis=1)\n",
    "\n",
    "# Take the new cluster labels and apply them to the users of our colour data\n",
    "# We will use these as classification labels.\n",
    "classification_feature= []\n",
    "for i in df['user']:\n",
    "    for j in X.index:\n",
    "        if i == j:\n",
    "            classification_feature.append(X['cluster_feature'].loc[i])\n",
    "\n",
    "df['cluster_classification'] = classification_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_sql('colour_classified', engine, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Classification\n",
    "\n",
    "CALIBRATION_GROUP = pd.DataFrame() # add all selections from loop to df.\n",
    "\n",
    "for pallet in df['pallet_used'].unique():\n",
    "\n",
    "    df_pallet = df[df['pallet_used'] == pallet] # iterate for the pallet.\n",
    "    \n",
    "    # cluster_classification'] == 0 : ncb\n",
    "    # cluster_classification'] == 1 : cb_type1\n",
    "    # cluster_classification'] == 2 : outliers\n",
    "    # cluster_classification'] == 3 : cb_type2\n",
    "\n",
    "    # Set for type1 colourblind correct\n",
    "    df1 = df_pallet[(df_pallet['cb_type1'] == 1) & (df_pallet['cluster_classification'] == 0) & (df_pallet['correct'] == 0)] \n",
    "    df2 = df_pallet[(df_pallet['cb_type1'] == 1) & (df_pallet['cluster_classification'] == 1) & (df_pallet['correct'] == 1)]\n",
    "    df3 = df_pallet[(df_pallet['cb_type1'] == 1) & (df_pallet['cluster_classification'] == 2) & (df_pallet['correct'] == 0)]\n",
    "    df4 = df_pallet[(df_pallet['cb_type1'] == 1) & (df_pallet['cluster_classification'] == 3) & (df_pallet['correct'] == 1)]\n",
    "\n",
    "    # Set for type2 colourblind correct\n",
    "    df5 = df_pallet[(df_pallet['cb_type2'] == 1) & (df_pallet['cluster_classification'] == 0) & (df_pallet['correct'] == 0)] \n",
    "    df6 = df_pallet[(df_pallet['cb_type2'] == 1) & (df_pallet['cluster_classification'] == 1) & (df_pallet['correct'] == 1)]\n",
    "    df7 = df_pallet[(df_pallet['cb_type2'] == 1) & (df_pallet['cluster_classification'] == 2) & (df_pallet['correct'] == 0)]\n",
    "    df8 = df_pallet[(df_pallet['cb_type2'] == 1) & (df_pallet['cluster_classification'] == 3) & (df_pallet['correct'] == 1)]\n",
    "\n",
    "    # Set for ncb correct\n",
    "    df9 = df_pallet[(df_pallet['ncb'] == 1) & (df_pallet['cluster_classification'] == 0) & (df_pallet['correct'] == 1)] \n",
    "    df10 = df_pallet[(df_pallet['ncb'] == 1) & (df_pallet['cluster_classification'] == 1) & (df_pallet['correct'] == 0)]\n",
    "    df11 = df_pallet[(df_pallet['ncb'] == 1) & (df_pallet['cluster_classification'] == 2) & (df_pallet['correct'] == 0)]\n",
    "    df12 = df_pallet[(df_pallet['ncb'] == 1) & (df_pallet['cluster_classification'] == 3) & (df_pallet['correct'] == 0)] \n",
    "\n",
    "    calibration = pd.concat([df1,df2,df3,df4,df5,df6,df7,df8,df9,df10,df11,df12],axis=0) #     This pulls together the criteria for improvement.\n",
    "    calibration = calibration[['pallet_used','ishihara_list']]\n",
    "\n",
    "    CALIBRATION_GROUP = pd.concat([CALIBRATION_GROUP,calibration],axis=0)\n",
    "\n",
    "CALIBRATION_GROUP = CALIBRATION_GROUP.reset_index()\n",
    "\n",
    "NEW_COLOURS_LIST = pd.DataFrame()\n",
    "PALLET_NAMES = calibration['pallet_used'].unique()\n",
    "CALIBRATION_REPORT = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_calibration_pallet(CALIBRATION_GROUP):\n",
    "    for pallet in CALIBRATION_GROUP['pallet_used'].unique(): # this cycles for each pallet used in the data individually\n",
    "        df_pallet = CALIBRATION_GROUP[CALIBRATION_GROUP['pallet_used'] == pallet] # iterate for the pallet.\n",
    "\n",
    "        if len(df_pallet['pallet_used']) > 30: # check there are enough samples\n",
    "            print(f\"{pallet} has {len(df_pallet['pallet_used'])} suitable responses for calibration.\")\n",
    "        else:\n",
    "            print(f\"WARNING: for {pallet} we have less than 30 samples. Skipping\")\n",
    "            continue\n",
    "\n",
    "        # Stage 1: Dictionary for iteration, made up of all colours.\n",
    "        colours = {\n",
    "            'colour_1' : {\n",
    "                'red' : [colour[0][0] for colour in df_pallet['ishihara_list']],\n",
    "                'green': [colour[0][1] for colour in df_pallet['ishihara_list']],\n",
    "                'blue' : [colour[0][2] for colour in df_pallet['ishihara_list']],\n",
    "            },\n",
    "            'colour_2' : {\n",
    "                'red' : [colour[1][0] for colour in df_pallet['ishihara_list']],\n",
    "                'green': [colour[1][1] for colour in df_pallet['ishihara_list']],\n",
    "                'blue' : [colour[1][2] for colour in df_pallet['ishihara_list']],\n",
    "            },\n",
    "            'colour_3' : {\n",
    "                'red' : [colour[2][0] for colour in df_pallet['ishihara_list']],\n",
    "                'green': [colour[2][1] for colour in df_pallet['ishihara_list']],\n",
    "                'blue' : [colour[2][2] for colour in df_pallet['ishihara_list']],\n",
    "            },\n",
    "            'colour_4' :{\n",
    "                'red' : [colour[3][0] for colour in df_pallet['ishihara_list']],\n",
    "                'green': [colour[3][1] for colour in df_pallet['ishihara_list']],\n",
    "                'blue' : [colour[3][2] for colour in df_pallet['ishihara_list']],\n",
    "            },\n",
    "            'colour_5' :{\n",
    "                'red' : [colour[4][0] for colour in df_pallet['ishihara_list']],\n",
    "                'green': [colour[4][1] for colour in df_pallet['ishihara_list']],\n",
    "                'blue' : [colour[4][2] for colour in df_pallet['ishihara_list']],\n",
    "            },\n",
    "            'colour_6' :{\n",
    "                'red' : [colour[5][0] for colour in df_pallet['ishihara_list']],\n",
    "                'green': [colour[5][1] for colour in df_pallet['ishihara_list']],\n",
    "                'blue' : [colour[5][2] for colour in df_pallet['ishihara_list']],\n",
    "            },\n",
    "            'colour_7' :{\n",
    "                'red' : [colour[6][0] for colour in df_pallet['ishihara_list']],\n",
    "                'green': [colour[6][1] for colour in df_pallet['ishihara_list']],\n",
    "                'blue' : [colour[6][2] for colour in df_pallet['ishihara_list']],\n",
    "            },\n",
    "            'colour_8' :{\n",
    "                'red' : [colour[7][0] for colour in df_pallet['ishihara_list']],\n",
    "                'green': [colour[7][1] for colour in df_pallet['ishihara_list']],\n",
    "                'blue' : [colour[7][2] for colour in df_pallet['ishihara_list']],\n",
    "            },\n",
    "            'colour_9' :{\n",
    "                'red' : [colour[8][0] for colour in df_pallet['ishihara_list']],\n",
    "                'green': [colour[8][1] for colour in df_pallet['ishihara_list']],\n",
    "                'blue' : [colour[8][2] for colour in df_pallet['ishihara_list']],\n",
    "            },\n",
    "            'colour_10' :{\n",
    "                'red' : [colour[9][0] for colour in df_pallet['ishihara_list']],\n",
    "                'green': [colour[9][1] for colour in df_pallet['ishihara_list']],\n",
    "                'blue' : [colour[9][2] for colour in df_pallet['ishihara_list']],\n",
    "            },\n",
    "            'colour_11' :{\n",
    "                'red' : [colour[10][0] for colour in df_pallet['ishihara_list']],\n",
    "                'green': [colour[10][1] for colour in df_pallet['ishihara_list']],\n",
    "                'blue' : [colour[10][2] for colour in df_pallet['ishihara_list']],\n",
    "            },\n",
    "            'colour_12' :{\n",
    "                'red' : [colour[11][0] for colour in df_pallet['ishihara_list']],\n",
    "                'green': [colour[11][1] for colour in df_pallet['ishihara_list']],\n",
    "                'blue' : [colour[11][2] for colour in df_pallet['ishihara_list']],\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Stage 2: Convert Dictionary to pandas DataFrame\n",
    "        coloursdf = pd.DataFrame(colours)\n",
    "\n",
    "        # Create List of Pairs For Mashing.\n",
    "        calibrated_pallet = []\n",
    "\n",
    "        pallet_report = {}\n",
    "\n",
    "        # Stage 3: Pairing & Clustering\n",
    "        for colour,rgb in coloursdf.items():\n",
    "            red_green = pd.DataFrame({'red': rgb[0],'green': rgb[1]})\n",
    "            green_blue = pd.DataFrame({'green': rgb[1],'blue': rgb[2]})\n",
    "            blue_red = pd.DataFrame({'blue': rgb[2],'red': rgb[0]})\n",
    "\n",
    "            calibrator = [red_green,green_blue,blue_red] # List of vectors between R,G,B  X,Y,Z points.\n",
    "\n",
    "            # Flexible DBSCAN Variables.\n",
    "            eps_val = 15 # EPS Max dist is 20, for spread 10. 10 = 50% of range\n",
    "            min_samples_val = int(len(calibrator[0].iloc[:, 0]) * .75) # 90% of the total pool required to make 1 cluster.\n",
    "\n",
    "            # Stage 4: DBSCAN\n",
    "            cluster_centers = [] # list of pair centers.\n",
    "            for pair in calibrator:\n",
    "                status = {}\n",
    "                # prune outliers\n",
    "                status['primary_count'] = len(pair.iloc[:, 0]) # Get data len # consider using .shape[0]\n",
    "                pair = pair[(np.abs(stats.zscore(pair.iloc[:, 0])) < 3)] # uses the zscore function of df less than a SD of 3\n",
    "                pair = pair[(np.abs(stats.zscore(pair.iloc[:, 1])) < 3)] # uses the zscore function of df less than a SD of 3\n",
    "                status['secondary_count'] = len(pair.iloc[:, 0]) # Get data len\n",
    "                status['primary_removed'] = status['primary_count'] - status['secondary_count'] # Get dif  in data len\n",
    "\n",
    "                dbscan = DBSCAN(eps=eps_val, min_samples=min_samples_val, metric = 'euclidean').fit_predict(pair) # create a big cluster\n",
    "                pair['dbscan'] = dbscan # assign it as an array\n",
    "                dbscan = pair[pair['dbscan'] == 0] # Take cluster 0 \n",
    "                dbscan = dbscan.drop('dbscan',axis=1)\n",
    "                status['third_count'] = len(dbscan.iloc[:, 0]) # Get data len\n",
    "                status['secondary_removed'] = status['secondary_count'] - status['third_count'] # Get dif  in data len\n",
    "\n",
    "                # KMeans, finding new cluster centers.\n",
    "                kmeans = KMeans(n_clusters=1).fit(dbscan) # fit KMEANS\n",
    "                cluster_centers.append(kmeans.cluster_centers_) # Take the center value of the cluster.\n",
    "\n",
    "                pallet_report[str(pair)] = status\n",
    "            CALIBRATION_REPORT[pallet] = pallet_report\n",
    "\n",
    "            red_corrected = int((cluster_centers[0][0][0] + cluster_centers[2][0][1]) / 2) # red\n",
    "            green_corrected = int((cluster_centers[0][0][1] + cluster_centers[1][0][0]) / 2) # green\n",
    "            blue_corrected = int((cluster_centers[1][0][1] + cluster_centers[2][0][0]) / 2) # blue\n",
    "\n",
    "            calibrated_rgb=[red_corrected,green_corrected,blue_corrected]\n",
    "            calibrated_rgb = '#%02x%02x%02x' % (red_corrected, green_corrected, blue_corrected) # Turn Tuple to Hex\n",
    "            calibrated_pallet.append(calibrated_rgb) # add the list of colours to a pallet list\n",
    "\n",
    "        NEW_COLOURS_LIST[pallet] = calibrated_pallet\n",
    "    return f'All pallets have been calibrated.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_calibration_pallet(CALIBRATION_GROUP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_COLOURS_LIST.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write back to pallet\n",
    "pallet_dict = pd.read_csv('./CSV/pallets_dictionary_new.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
